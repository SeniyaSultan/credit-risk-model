# -*- coding: utf-8 -*-
"""data_processing.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mp0BJu_4MgJahinfXSCfpRBItTfh1zfn
"""

import pandas as pd
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer

def create_aggregate_features(df: pd.DataFrame) -> pd.DataFrame:
    agg = df.groupby("CustomerId").agg(
        total_amount=("Amount", "sum"),
        avg_amount=("Amount", "mean"),
        transaction_count=("TransactionId", "count"),
        std_amount=("Amount", "std")
    ).reset_index()

    return agg

def extract_time_features(df: pd.DataFrame) -> pd.DataFrame:
    df["TransactionStartTime"] = pd.to_datetime(df["TransactionStartTime"])

    df["transaction_hour"] = df["TransactionStartTime"].dt.hour
    df["transaction_day"] = df["TransactionStartTime"].dt.day
    df["transaction_month"] = df["TransactionStartTime"].dt.month
    df["transaction_year"] = df["TransactionStartTime"].dt.year

    return df

NUM_COLS = [
    "total_amount",
    "avg_amount",
    "transaction_count",
    "std_amount"
]

CAT_COLS = [
    "ProductCategory",
    "ChannelId"
]

num_pipeline = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])

cat_pipeline = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("encoder", OneHotEncoder(handle_unknown="ignore"))
])

preprocessor = ColumnTransformer(transformers=[
    ("num", num_pipeline, NUM_COLS),
    ("cat", cat_pipeline, CAT_COLS)
])

def build_feature_pipeline():
    return preprocessor

"""Feature engineering was implemented using a reproducible sklearn Pipeline. Customer-level aggregate features were created to capture behavioral risk signals, temporal features were extracted from transaction timestamps, categorical variables were encoded using One-Hot Encoding, and numerical variables were standardized to ensure scale consistency. This approach ensures production readiness and seamless integration with model training and deployment"""

from datetime import timedelta
import pandas as pd

def compute_rfm(df: pd.DataFrame) -> pd.DataFrame:
    df["TransactionStartTime"] = pd.to_datetime(df["TransactionStartTime"])

    snapshot_date = df["TransactionStartTime"].max() + timedelta(days=1)

    rfm = df.groupby("CustomerId").agg(
        recency=("TransactionStartTime", lambda x: (snapshot_date - x.max()).days),
        frequency=("TransactionId", "count"),
        monetary=("Amount", "sum")
    ).reset_index()

    return rfm

from sklearn.preprocessing import StandardScaler

def scale_rfm(rfm_df: pd.DataFrame) -> pd.DataFrame:
    scaler = StandardScaler()
    rfm_scaled = scaler.fit_transform(
        rfm_df[["recency", "frequency", "monetary"]]
    )

    return rfm_scaled

from sklearn.cluster import KMeans

def cluster_customers(rfm_scaled, n_clusters=3):
    kmeans = KMeans(
        n_clusters=n_clusters,
        random_state=42,
        n_init=10
    )
    return kmeans.fit_predict(rfm_scaled)

def assign_risk_label(rfm: pd.DataFrame, cluster_labels) -> pd.DataFrame:
    rfm["cluster"] = cluster_labels

    cluster_summary = rfm.groupby("cluster").mean()

    high_risk_cluster = cluster_summary.sort_values(
        by=["frequency", "monetary"]
    ).index[0]

    rfm["is_high_risk"] = (rfm["cluster"] == high_risk_cluster).astype(int)

    return rfm[["CustomerId", "is_high_risk"]]

def merge_target(df: pd.DataFrame, target_df: pd.DataFrame) -> pd.DataFrame:
    return df.merge(target_df, on="CustomerId", how="left")

def build_proxy_target(df: pd.DataFrame) -> pd.DataFrame:
    rfm = compute_rfm(df)
    rfm_scaled = scale_rfm(rfm)
    clusters = cluster_customers(rfm_scaled)
    target = assign_risk_label(rfm, clusters)
    return merge_target(df, target)

"""“Since the dataset does not contain an explicit default indicator, a proxy target variable was engineered using Recency, Frequency, and Monetary (RFM) analysis. Customers were clustered into three segments using K-Means after standardizing RFM features. The least engaged cluster—characterized by high recency, low transaction frequency, and low monetary value—was labeled as high-risk. This binary proxy target (is_high_risk) was then merged back into the processed dataset for supervised model training.”"""